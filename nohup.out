params_array: test
current_task: test
/data/wzw/wzw/BitDistiller-Q4_0
loading llm model /data/wzw/models/Llama-2-7b-chat-hf-0.7-static-STE/ckpts/Llama-2-7b-chat-hf/int4-g64/checkpoint-280
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.09s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.01s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.17it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.11it/s]
model.hf_device_map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
begin model sparse hook setting...
Create and load preditor...
Local device: cuda:0
Init Reset
num_layers 32
weight_counters {0: 7, 1: 7, 2: 7, 3: 7, 4: 7, 5: 7, 6: 7, 7: 7, 8: 7, 9: 7, 10: 7, 11: 7, 12: 7, 13: 7, 14: 7, 15: 7, 16: 7, 17: 7, 18: 7, 19: 7, 20: 7, 21: 7, 22: 7, 23: 7, 24: 7, 25: 7, 26: 7, 27: 7, 28: 7, 29: 7, 30: 7, 31: 7}
Init Reset
Filtered model elements weight-name / weight-id
weight_id 0 weight_name:  model.layers.0.self_attn.q_proj
weight_id 1 weight_name:  model.layers.0.self_attn.k_proj
weight_id 2 weight_name:  model.layers.0.self_attn.v_proj
weight_id 3 weight_name:  model.layers.0.self_attn.o_proj
weight_id 4 weight_name:  model.layers.0.mlp.gate_proj
weight_id 5 weight_name:  model.layers.0.mlp.up_proj
weight_id 6 weight_name:  model.layers.0.mlp.down_proj
Quantizing:   0%|          | 0/32 [00:00<?, ?it/s]Quantizing:   3%|▎         | 1/32 [00:00<00:03,  8.22it/s]Quantizing: 100%|██████████| 32/32 [00:00<00:00, 170.29it/s]Quantizing: 100%|██████████| 32/32 [00:00<00:00, 143.57it/s]
2025-03-31:13:33:05,662 WARNING  [huggingface.py:122] `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
2025-03-31:13:33:06,518 WARNING  [huggingface.py:329] Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
2025-03-31:13:33:06,519 INFO     [evaluator.py:152] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2025-03-31:13:33:06,519 INFO     [evaluator.py:203] Using pre-initialized model
2025-03-31:13:33:20,869 WARNING  [evaluator.py:251] Overwriting default num_fewshot of gsm8k from 5 to 5
2025-03-31:13:33:20,869 INFO     [evaluator.py:261] Setting fewshot random generator seed to 1234
2025-03-31:13:33:20,870 INFO     [task.py:411] Building contexts for gsm8k on rank 0...
task: eval
task_name: ['wiki']
task_list ['gsm8k']
sp_configs:  [(0.0, 0.0, 0.0, 0)]
begin : sp_config:  (0.0, 0.0, 0.0, 0) num_shot:  5
Init Reset
Set sparsity: attn 0.0, mlp 0.0, w 0.0
Set pre-prediction: 0
set_sparsity_threshold
threshold_path /data/wzw/wzw/BitDistiller-Q4_0/data/threshold/Meta-Llama-3-8B/sparse-0.json
sparsity_strategy :  Dynamic
sparsity_strategy:  Static
  0%|          | 0/300 [00:00<?, ?it/s]  7%|▋         | 22/300 [00:00<00:01, 212.62it/s] 15%|█▍        | 44/300 [00:00<00:01, 209.01it/s] 22%|██▏       | 66/300 [00:00<00:01, 211.22it/s] 29%|██▉       | 88/300 [00:00<00:01, 211.64it/s] 37%|███▋      | 110/300 [00:00<00:00, 212.38it/s] 44%|████▍     | 132/300 [00:00<00:00, 213.91it/s] 51%|█████▏    | 154/300 [00:00<00:00, 214.68it/s] 59%|█████▊    | 176/300 [00:00<00:00, 213.56it/s] 66%|██████▌   | 198/300 [00:00<00:00, 214.28it/s] 73%|███████▎  | 220/300 [00:01<00:00, 212.84it/s] 81%|████████  | 242/300 [00:01<00:00, 213.29it/s] 88%|████████▊ | 264/300 [00:01<00:00, 214.39it/s] 95%|█████████▌| 286/300 [00:01<00:00, 214.72it/s]100%|██████████| 300/300 [00:01<00:00, 213.46it/s]
2025-03-31:13:33:22,284 INFO     [evaluator.py:438] Running generate_until requests
Running generate_until requests:   0%|          | 0/300 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1156 > 512). Running this sequence through the model will result in indexing errors
/home/donglinbai/miniconda3/envs/sparse/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/donglinbai/miniconda3/envs/sparse/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Running generate_until requests:   0%|          | 1/300 [00:11<57:48, 11.60s/it]Running generate_until requests:   1%|          | 2/300 [00:26<1:07:50, 13.66s/it]Running generate_until requests:   1%|          | 3/300 [00:37<1:00:21, 12.19s/it]Running generate_until requests:   1%|▏         | 4/300 [00:43<47:57,  9.72s/it]  Running generate_until requests:   2%|▏         | 5/300 [01:01<1:03:04, 12.83s/it]Running generate_until requests:   2%|▏         | 6/300 [01:19<1:12:01, 14.70s/it]params_array: test
current_task: test
/data/wzw/wzw/BitDistiller-Q4_0
loading llm model /data/wzw/models/Meta-Llama-3-8B
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.09s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.12s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.11s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.26it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.10it/s]
model.hf_device_map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 2, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 3, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
)
begin model sparse hook setting...
Create and load preditor...
Local device: cuda:0
Init Reset
num_layers 32
weight_counters {0: 7, 1: 7, 2: 7, 3: 7, 4: 7, 5: 7, 6: 7, 7: 7, 8: 7, 9: 7, 10: 7, 11: 7, 12: 7, 13: 7, 14: 7, 15: 7, 16: 7, 17: 7, 18: 7, 19: 7, 20: 7, 21: 7, 22: 7, 23: 7, 24: 7, 25: 7, 26: 7, 27: 7, 28: 7, 29: 7, 30: 7, 31: 7}
Init Reset
Filtered model elements weight-name / weight-id
weight_id 0 weight_name:  model.layers.0.self_attn.q_proj
weight_id 1 weight_name:  model.layers.0.self_attn.k_proj
weight_id 2 weight_name:  model.layers.0.self_attn.v_proj
weight_id 3 weight_name:  model.layers.0.self_attn.o_proj
weight_id 4 weight_name:  model.layers.0.mlp.gate_proj
weight_id 5 weight_name:  model.layers.0.mlp.up_proj
weight_id 6 weight_name:  model.layers.0.mlp.down_proj
Quantizing:   0%|          | 0/32 [00:00<?, ?it/s]Quantizing:   3%|▎         | 1/32 [00:00<00:03,  7.92it/s]Quantizing:  16%|█▌        | 5/32 [00:00<00:01, 24.91it/s]
Traceback (most recent call last):
  File "/data/wzw/wzw/BitDistiller-Q4_0/train/test_task.py", line 268, in <module>
    main()
  File "/data/wzw/wzw/BitDistiller-Q4_0/train/test_task.py", line 217, in main
    quant_and_dequant_model_q4_0(model)
  File "/data/wzw/wzw/BitDistiller-Q4_0/quantization/qlinear.py", line 177, in quant_and_dequant_model_q4_0
    quant_and_dequant_tensor_q4_0_v2(weight, do_transpose)
  File "/data/wzw/wzw/BitDistiller-Q4_0/quantization/qlinear.py", line 136, in quant_and_dequant_tensor_q4_0_v2
    x = Floor.apply(torch.minimum(torch.ones_like(x, device=x.device) * 15.0, x * (1.0 / abs_max_val) + 8.5))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 1 has a total capacty of 23.64 GiB of which 45.69 MiB is free. Process 3523280 has 13.38 GiB memory in use. Process 3531848 has 5.88 GiB memory in use. Including non-PyTorch memory, this process has 4.32 GiB memory in use. Of the allocated memory 3.85 GiB is allocated by PyTorch, and 33.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Running generate_until requests:   2%|▏         | 7/300 [01:38<1:18:21, 16.05s/it]Running generate_until requests:   3%|▎         | 8/300 [01:47<1:07:25, 13.85s/it]Running generate_until requests:   3%|▎         | 9/300 [02:06<1:14:08, 15.29s/it]Running generate_until requests:   3%|▎         | 10/300 [02:14<1:03:46, 13.20s/it]Running generate_until requests:   4%|▎         | 11/300 [02:28<1:04:26, 13.38s/it]Running generate_until requests:   4%|▍         | 12/300 [02:46<1:11:43, 14.94s/it]Running generate_until requests:   4%|▍         | 13/300 [02:57<1:05:09, 13.62s/it]Running generate_until requests:   5%|▍         | 14/300 [03:09<1:01:50, 12.97s/it]Running generate_until requests:   5%|▌         | 15/300 [03:15<52:31, 11.06s/it]  Running generate_until requests:   5%|▌         | 16/300 [03:22<46:30,  9.83s/it]Running generate_until requests:   6%|▌         | 17/300 [03:30<44:07,  9.36s/it]Running generate_until requests:   6%|▌         | 18/300 [03:42<47:15, 10.06s/it]Running generate_until requests:   6%|▋         | 19/300 [03:50<43:34,  9.31s/it]Running generate_until requests:   7%|▋         | 20/300 [03:57<41:14,  8.84s/it]Running generate_until requests:   7%|▋         | 21/300 [04:03<36:55,  7.94s/it]Running generate_until requests:   7%|▋         | 22/300 [04:20<48:34, 10.48s/it]Running generate_until requests:   8%|▊         | 23/300 [04:38<58:59, 12.78s/it]Running generate_until requests:   8%|▊         | 24/300 [04:44<49:30, 10.76s/it]Running generate_until requests:   8%|▊         | 25/300 [04:56<51:16, 11.19s/it]Running generate_until requests:   9%|▊         | 26/300 [05:12<57:49, 12.66s/it]tools/test_task.sh: line 18: 3531848 Killed                  python test_task.py --model=${MODEL_PATH} --seed=42 --task=${TEST_TASK} --sparse=${SPARSE} --limit=300 --num_shot=0 --do_cr=${DO_CR} --threshold_path=${THRESHOLD_PATH} --sparse_strategy=${SPARSE_STRATEGY} --batch_size=1 --quant=1 --test_all=${TEST_ALL}
tools/run_wrapper.sh: line 50: ne: command not found
params_array: test
current_task: test
/data/wzw/wzw/BitDistiller-Q4_0
loading llm model /data/wzw/models/Meta-Llama-3-8B
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.07s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.10s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.09s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.28it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.12it/s]
model.hf_device_map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
)
begin model sparse hook setting...
Create and load preditor...
Local device: cuda:0
Init Reset
num_layers 32
weight_counters {0: 7, 1: 7, 2: 7, 3: 7, 4: 7, 5: 7, 6: 7, 7: 7, 8: 7, 9: 7, 10: 7, 11: 7, 12: 7, 13: 7, 14: 7, 15: 7, 16: 7, 17: 7, 18: 7, 19: 7, 20: 7, 21: 7, 22: 7, 23: 7, 24: 7, 25: 7, 26: 7, 27: 7, 28: 7, 29: 7, 30: 7, 31: 7}
Init Reset
Filtered model elements weight-name / weight-id
weight_id 0 weight_name:  model.layers.0.self_attn.q_proj
weight_id 1 weight_name:  model.layers.0.self_attn.k_proj
weight_id 2 weight_name:  model.layers.0.self_attn.v_proj
weight_id 3 weight_name:  model.layers.0.self_attn.o_proj
weight_id 4 weight_name:  model.layers.0.mlp.gate_proj
weight_id 5 weight_name:  model.layers.0.mlp.up_proj
weight_id 6 weight_name:  model.layers.0.mlp.down_proj
Quantizing:   0%|          | 0/32 [00:00<?, ?it/s]Quantizing:   3%|▎         | 1/32 [00:00<00:04,  7.22it/s]Quantizing:  81%|████████▏ | 26/32 [00:00<00:00, 120.75it/s]Quantizing: 100%|██████████| 32/32 [00:00<00:00, 121.09it/s]
2025-03-31:13:41:00,246 WARNING  [huggingface.py:122] `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
2025-03-31:13:41:00,944 WARNING  [huggingface.py:329] Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
2025-03-31:13:41:00,946 INFO     [evaluator.py:152] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2025-03-31:13:41:00,946 INFO     [evaluator.py:203] Using pre-initialized model
2025-03-31:13:41:15,384 WARNING  [evaluator.py:251] Overwriting default num_fewshot of gsm8k from 5 to 5
2025-03-31:13:41:15,384 INFO     [evaluator.py:261] Setting fewshot random generator seed to 1234
2025-03-31:13:41:15,385 INFO     [task.py:411] Building contexts for gsm8k on rank 0...
task: eval
task_name: ['wiki']
task_list ['gsm8k']
sp_configs:  [(0.0, 0.0, 0.0, 0)]
begin : sp_config:  (0.0, 0.0, 0.0, 0) num_shot:  5
Init Reset
Set sparsity: attn 0.0, mlp 0.0, w 0.0
Set pre-prediction: 0
set_sparsity_threshold
threshold_path /data/wzw/wzw/BitDistiller-Q4_0/data/threshold/Meta-Llama-3-8B/sparse-0.json
sparsity_strategy :  Dynamic
sparsity_strategy:  Static
  0%|          | 0/300 [00:00<?, ?it/s]  7%|▋         | 22/300 [00:00<00:01, 214.11it/s] 15%|█▍        | 44/300 [00:00<00:01, 214.59it/s] 22%|██▏       | 66/300 [00:00<00:01, 211.55it/s] 29%|██▉       | 88/300 [00:00<00:00, 213.24it/s] 37%|███▋      | 110/300 [00:00<00:00, 214.41it/s] 44%|████▍     | 132/300 [00:00<00:00, 215.90it/s] 51%|█████▏    | 154/300 [00:00<00:00, 216.26it/s] 59%|█████▊    | 176/300 [00:00<00:00, 216.18it/s] 66%|██████▌   | 198/300 [00:00<00:00, 216.99it/s] 73%|███████▎  | 220/300 [00:01<00:00, 217.06it/s] 81%|████████  | 242/300 [00:01<00:00, 216.83it/s] 88%|████████▊ | 264/300 [00:01<00:00, 216.58it/s] 95%|█████████▌| 286/300 [00:01<00:00, 216.52it/s]100%|██████████| 300/300 [00:01<00:00, 215.62it/s]
2025-03-31:13:41:16,785 INFO     [evaluator.py:438] Running generate_until requests
Running generate_until requests:   0%|          | 0/300 [00:00<?, ?it/s]/home/donglinbai/miniconda3/envs/sparse/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/donglinbai/miniconda3/envs/sparse/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Running generate_until requests:   0%|          | 1/300 [00:08<43:26,  8.72s/it]Running generate_until requests:   1%|          | 2/300 [00:11<26:37,  5.36s/it]Running generate_until requests:   1%|          | 3/300 [00:20<35:18,  7.13s/it]Running generate_until requests:   1%|▏         | 4/300 [00:27<33:06,  6.71s/it]Running generate_until requests:   2%|▏         | 5/300 [00:40<45:34,  9.27s/it]Running generate_until requests:   2%|▏         | 6/300 [00:49<44:41,  9.12s/it]Running generate_until requests:   2%|▏         | 7/300 [01:09<1:01:35, 12.61s/it]Running generate_until requests:   3%|▎         | 8/300 [01:16<53:02, 10.90s/it]  Running generate_until requests:   3%|▎         | 9/300 [01:25<49:18, 10.17s/it]Running generate_until requests:   3%|▎         | 10/300 [01:30<41:45,  8.64s/it]Running generate_until requests:   4%|▎         | 11/300 [01:45<51:31, 10.70s/it]Running generate_until requests:   4%|▍         | 12/300 [01:51<43:59,  9.16s/it]Running generate_until requests:   4%|▍         | 13/300 [01:56<37:59,  7.94s/it]Running generate_until requests:   5%|▍         | 14/300 [02:06<41:13,  8.65s/it]Running generate_until requests:   5%|▌         | 15/300 [02:15<40:17,  8.48s/it]Running generate_until requests:   5%|▌         | 16/300 [02:27<46:32,  9.83s/it]Running generate_until requests:   6%|▌         | 17/300 [02:34<42:16,  8.96s/it]Running generate_until requests:   6%|▌         | 18/300 [02:40<37:39,  8.01s/it]Running generate_until requests:   6%|▋         | 19/300 [02:52<42:49,  9.14s/it]Running generate_until requests:   7%|▋         | 20/300 [03:06<49:18, 10.57s/it]Running generate_until requests:   7%|▋         | 21/300 [03:14<45:17,  9.74s/it]Running generate_until requests:   7%|▋         | 22/300 [03:22<43:30,  9.39s/it]Running generate_until requests:   8%|▊         | 23/300 [03:31<42:55,  9.30s/it]Running generate_until requests:   8%|▊         | 24/300 [03:36<36:26,  7.92s/it]Running generate_until requests:   8%|▊         | 25/300 [03:40<30:15,  6.60s/it]Running generate_until requests:   9%|▊         | 26/300 [03:45<28:21,  6.21s/it]Running generate_until requests:   9%|▉         | 27/300 [03:50<26:41,  5.87s/it]Running generate_until requests:   9%|▉         | 28/300 [03:58<29:16,  6.46s/it]Running generate_until requests:  10%|▉         | 29/300 [04:06<31:22,  6.95s/it]Running generate_until requests:  10%|█         | 30/300 [04:16<35:14,  7.83s/it]Running generate_until requests:  10%|█         | 31/300 [04:21<31:29,  7.02s/it]Running generate_until requests:  11%|█         | 32/300 [04:31<34:55,  7.82s/it]Running generate_until requests:  11%|█         | 33/300 [04:46<44:52, 10.08s/it]Running generate_until requests:  11%|█▏        | 34/300 [05:04<54:47, 12.36s/it]Running generate_until requests:  12%|█▏        | 35/300 [05:10<46:00, 10.42s/it]Running generate_until requests:  12%|█▏        | 36/300 [05:29<57:49, 13.14s/it]Running generate_until requests:  12%|█▏        | 37/300 [05:34<47:26, 10.82s/it]Running generate_until requests:  13%|█▎        | 38/300 [05:48<51:18, 11.75s/it]Running generate_until requests:  13%|█▎        | 39/300 [05:51<39:16,  9.03s/it]Running generate_until requests:  13%|█▎        | 40/300 [05:59<37:29,  8.65s/it]Running generate_until requests:  14%|█▎        | 41/300 [06:01<29:30,  6.83s/it]Running generate_until requests:  14%|█▍        | 42/300 [06:12<34:54,  8.12s/it]Running generate_until requests:  14%|█▍        | 43/300 [06:22<36:43,  8.57s/it]Running generate_until requests:  15%|█▍        | 44/300 [06:40<47:55, 11.23s/it]Running generate_until requests:  15%|█▌        | 45/300 [06:59<58:43, 13.82s/it]Running generate_until requests:  15%|█▌        | 46/300 [07:18<1:04:41, 15.28s/it]Running generate_until requests:  16%|█▌        | 47/300 [07:23<51:10, 12.14s/it]  Running generate_until requests:  16%|█▌        | 48/300 [07:28<42:07, 10.03s/it]Running generate_until requests:  16%|█▋        | 49/300 [07:39<42:45, 10.22s/it]Running generate_until requests:  17%|█▋        | 50/300 [07:46<38:36,  9.27s/it]Running generate_until requests:  17%|█▋        | 51/300 [07:51<33:19,  8.03s/it]Running generate_until requests:  17%|█▋        | 52/300 [07:57<30:57,  7.49s/it]Running generate_until requests:  18%|█▊        | 53/300 [08:05<31:38,  7.68s/it]Running generate_until requests:  18%|█▊        | 54/300 [08:17<35:56,  8.77s/it]Running generate_until requests:  18%|█▊        | 55/300 [08:22<31:16,  7.66s/it]Running generate_until requests:  19%|█▊        | 56/300 [08:36<39:49,  9.79s/it]Running generate_until requests:  19%|█▉        | 57/300 [08:47<40:23,  9.97s/it]Running generate_until requests:  19%|█▉        | 58/300 [08:55<37:57,  9.41s/it]Running generate_until requests:  20%|█▉        | 59/300 [09:09<44:00, 10.96s/it]Running generate_until requests:  20%|██        | 60/300 [09:19<42:11, 10.55s/it]Running generate_until requests:  20%|██        | 61/300 [09:29<41:45, 10.49s/it]Running generate_until requests:  21%|██        | 62/300 [09:34<34:48,  8.78s/it]Running generate_until requests:  21%|██        | 63/300 [09:42<33:53,  8.58s/it]Running generate_until requests:  21%|██▏       | 64/300 [09:54<37:30,  9.54s/it]Running generate_until requests:  22%|██▏       | 65/300 [10:01<34:51,  8.90s/it]Running generate_until requests:  22%|██▏       | 66/300 [10:11<35:02,  8.99s/it]Running generate_until requests:  22%|██▏       | 67/300 [10:23<38:19,  9.87s/it]Running generate_until requests:  23%|██▎       | 68/300 [10:29<34:13,  8.85s/it]Running generate_until requests:  23%|██▎       | 69/300 [10:39<35:00,  9.09s/it]Running generate_until requests:  23%|██▎       | 70/300 [10:50<37:33,  9.80s/it]Running generate_until requests:  24%|██▎       | 71/300 [10:56<32:56,  8.63s/it]Running generate_until requests:  24%|██▍       | 72/300 [11:04<32:09,  8.46s/it]Running generate_until requests:  24%|██▍       | 73/300 [11:11<30:08,  7.97s/it]Running generate_until requests:  25%|██▍       | 74/300 [11:17<28:24,  7.54s/it]Running generate_until requests:  25%|██▌       | 75/300 [11:24<27:01,  7.21s/it]Running generate_until requests:  25%|██▌       | 76/300 [11:37<33:30,  8.97s/it]Running generate_until requests:  26%|██▌       | 77/300 [11:57<45:09, 12.15s/it]Running generate_until requests:  26%|██▌       | 78/300 [12:03<39:05, 10.57s/it]Running generate_until requests:  26%|██▋       | 79/300 [12:12<37:13, 10.11s/it]Running generate_until requests:  27%|██▋       | 80/300 [12:23<37:13, 10.15s/it]Running generate_until requests:  27%|██▋       | 81/300 [12:30<33:36,  9.21s/it]Running generate_until requests:  27%|██▋       | 82/300 [12:35<28:57,  7.97s/it]Running generate_until requests:  28%|██▊       | 83/300 [12:39<25:15,  6.98s/it]Running generate_until requests:  28%|██▊       | 84/300 [12:53<32:17,  8.97s/it]Running generate_until requests:  28%|██▊       | 85/300 [13:00<29:55,  8.35s/it]Running generate_until requests:  29%|██▊       | 86/300 [13:06<27:00,  7.57s/it]Running generate_until requests:  29%|██▉       | 87/300 [13:25<39:30, 11.13s/it]Running generate_until requests:  29%|██▉       | 88/300 [13:38<41:24, 11.72s/it]Running generate_until requests:  30%|██▉       | 89/300 [13:43<34:08,  9.71s/it]Running generate_until requests:  30%|███       | 90/300 [13:49<30:12,  8.63s/it]Running generate_until requests:  30%|███       | 91/300 [13:57<29:08,  8.37s/it]Running generate_until requests:  31%|███       | 92/300 [14:02<25:20,  7.31s/it]Running generate_until requests:  31%|███       | 93/300 [14:07<22:59,  6.66s/it]Running generate_until requests:  31%|███▏      | 94/300 [14:11<20:15,  5.90s/it]Running generate_until requests:  32%|███▏      | 95/300 [14:25<28:25,  8.32s/it]Running generate_until requests:  32%|███▏      | 96/300 [14:32<26:35,  7.82s/it]Running generate_until requests:  32%|███▏      | 97/300 [14:37<23:54,  7.07s/it]Running generate_until requests:  33%|███▎      | 98/300 [14:43<22:21,  6.64s/it]Running generate_until requests:  33%|███▎      | 99/300 [14:49<22:11,  6.63s/it]Running generate_until requests:  33%|███▎      | 100/300 [14:58<24:02,  7.21s/it]Running generate_until requests:  34%|███▎      | 101/300 [15:11<29:20,  8.85s/it]Running generate_until requests:  34%|███▍      | 102/300 [15:16<25:31,  7.73s/it]Running generate_until requests:  34%|███▍      | 103/300 [15:20<21:54,  6.67s/it]Running generate_until requests:  35%|███▍      | 104/300 [15:28<23:24,  7.16s/it]Running generate_until requests:  35%|███▌      | 105/300 [15:35<22:18,  6.86s/it]Running generate_until requests:  35%|███▌      | 106/300 [15:54<34:30, 10.67s/it]Running generate_until requests:  36%|███▌      | 107/300 [16:02<31:41,  9.85s/it]Running generate_until requests:  36%|███▌      | 108/300 [16:05<24:31,  7.66s/it]Running generate_until requests:  36%|███▋      | 109/300 [16:13<25:16,  7.94s/it]Running generate_until requests:  37%|███▋      | 110/300 [16:28<31:35,  9.97s/it]Running generate_until requests:  37%|███▋      | 111/300 [16:33<26:57,  8.56s/it]Running generate_until requests:  37%|███▋      | 112/300 [16:39<24:38,  7.86s/it]Running generate_until requests:  38%|███▊      | 113/300 [16:50<27:07,  8.70s/it]Running generate_until requests:  38%|███▊      | 114/300 [16:56<24:02,  7.75s/it]Running generate_until requests:  38%|███▊      | 115/300 [17:03<23:44,  7.70s/it]Running generate_until requests:  39%|███▊      | 116/300 [17:10<23:05,  7.53s/it]Running generate_until requests:  39%|███▉      | 117/300 [17:15<20:26,  6.70s/it]Running generate_until requests:  39%|███▉      | 118/300 [17:20<18:40,  6.16s/it]Running generate_until requests:  40%|███▉      | 119/300 [17:26<18:07,  6.01s/it]Running generate_until requests:  40%|████      | 120/300 [17:31<17:08,  5.72s/it]Running generate_until requests:  40%|████      | 121/300 [17:40<20:25,  6.84s/it]Running generate_until requests:  41%|████      | 122/300 [17:45<18:52,  6.36s/it]Running generate_until requests:  41%|████      | 123/300 [17:59<25:17,  8.57s/it]Running generate_until requests:  41%|████▏     | 124/300 [18:07<24:11,  8.24s/it]Running generate_until requests:  42%|████▏     | 125/300 [18:11<20:26,  7.01s/it]Running generate_until requests:  42%|████▏     | 126/300 [18:15<17:33,  6.06s/it]Running generate_until requests:  42%|████▏     | 127/300 [18:18<14:56,  5.18s/it]Running generate_until requests:  43%|████▎     | 128/300 [18:24<15:39,  5.46s/it]Running generate_until requests:  43%|████▎     | 129/300 [18:33<18:53,  6.63s/it]Running generate_until requests:  43%|████▎     | 130/300 [18:40<18:41,  6.60s/it]Running generate_until requests:  44%|████▎     | 131/300 [18:43<16:10,  5.74s/it]Running generate_until requests:  44%|████▍     | 132/300 [18:47<13:58,  4.99s/it]Running generate_until requests:  44%|████▍     | 133/300 [18:55<16:35,  5.96s/it]Running generate_until requests:  45%|████▍     | 134/300 [18:59<15:17,  5.53s/it]Running generate_until requests:  45%|████▌     | 135/300 [19:07<16:49,  6.12s/it]Running generate_until requests:  45%|████▌     | 136/300 [19:11<15:20,  5.61s/it]Running generate_until requests:  46%|████▌     | 137/300 [19:17<15:27,  5.69s/it]Running generate_until requests:  46%|████▌     | 138/300 [19:27<18:20,  6.79s/it]Running generate_until requests:  46%|████▋     | 139/300 [19:32<16:55,  6.31s/it]Running generate_until requests:  47%|████▋     | 140/300 [19:39<17:13,  6.46s/it]Running generate_until requests:  47%|████▋     | 141/300 [19:42<15:07,  5.71s/it]Running generate_until requests:  47%|████▋     | 142/300 [19:52<18:08,  6.89s/it]Running generate_until requests:  48%|████▊     | 143/300 [19:58<17:13,  6.58s/it]Running generate_until requests:  48%|████▊     | 144/300 [20:05<17:35,  6.77s/it]Running generate_until requests:  48%|████▊     | 145/300 [20:19<23:00,  8.91s/it]Running generate_until requests:  49%|████▊     | 146/300 [20:23<19:13,  7.49s/it]Running generate_until requests:  49%|████▉     | 147/300 [20:31<19:01,  7.46s/it]Running generate_until requests:  49%|████▉     | 148/300 [20:37<18:21,  7.25s/it]Running generate_until requests:  50%|████▉     | 149/300 [20:47<19:54,  7.91s/it]Running generate_until requests:  50%|█████     | 150/300 [20:52<17:27,  6.98s/it]Running generate_until requests:  50%|█████     | 151/300 [20:57<15:47,  6.36s/it]Running generate_until requests:  51%|█████     | 152/300 [21:07<18:50,  7.64s/it]Running generate_until requests:  51%|█████     | 153/300 [21:14<17:55,  7.32s/it]Running generate_until requests:  51%|█████▏    | 154/300 [21:24<19:36,  8.06s/it]Running generate_until requests:  52%|█████▏    | 155/300 [21:31<18:39,  7.72s/it]Running generate_until requests:  52%|█████▏    | 156/300 [21:41<20:29,  8.54s/it]Running generate_until requests:  52%|█████▏    | 157/300 [21:51<21:23,  8.98s/it]Running generate_until requests:  53%|█████▎    | 158/300 [22:00<21:02,  8.89s/it]Running generate_until requests:  53%|█████▎    | 159/300 [22:14<24:35, 10.46s/it]Running generate_until requests:  53%|█████▎    | 160/300 [22:23<23:36, 10.12s/it]Running generate_until requests:  54%|█████▎    | 161/300 [22:31<22:09,  9.56s/it]Running generate_until requests:  54%|█████▍    | 162/300 [22:42<22:58,  9.99s/it]Running generate_until requests:  54%|█████▍    | 163/300 [22:48<20:06,  8.81s/it]Running generate_until requests:  55%|█████▍    | 164/300 [22:58<20:12,  8.92s/it]Running generate_until requests:  55%|█████▌    | 165/300 [23:04<18:39,  8.29s/it]Running generate_until requests:  55%|█████▌    | 166/300 [23:11<17:30,  7.84s/it]Running generate_until requests:  56%|█████▌    | 167/300 [23:17<16:13,  7.32s/it]Running generate_until requests:  56%|█████▌    | 168/300 [23:26<16:46,  7.62s/it]Running generate_until requests:  56%|█████▋    | 169/300 [23:33<16:23,  7.51s/it]Running generate_until requests:  57%|█████▋    | 170/300 [23:40<15:57,  7.36s/it]Running generate_until requests:  57%|█████▋    | 171/300 [23:50<17:29,  8.14s/it]Running generate_until requests:  57%|█████▋    | 172/300 [23:57<17:01,  7.98s/it]Running generate_until requests:  58%|█████▊    | 173/300 [24:06<17:00,  8.03s/it]Running generate_until requests:  58%|█████▊    | 174/300 [24:13<16:14,  7.74s/it]Running generate_until requests:  58%|█████▊    | 175/300 [24:22<17:23,  8.35s/it]Running generate_until requests:  59%|█████▊    | 176/300 [24:33<18:36,  9.01s/it]Running generate_until requests:  59%|█████▉    | 177/300 [24:38<16:05,  7.85s/it]Running generate_until requests:  59%|█████▉    | 178/300 [24:50<18:09,  8.93s/it]Running generate_until requests:  60%|█████▉    | 179/300 [24:58<17:48,  8.83s/it]Running generate_until requests:  60%|██████    | 180/300 [25:07<17:33,  8.78s/it]Running generate_until requests:  60%|██████    | 181/300 [25:16<17:23,  8.77s/it]Running generate_until requests:  61%|██████    | 182/300 [25:25<17:21,  8.83s/it]Running generate_until requests:  61%|██████    | 183/300 [25:30<15:16,  7.84s/it]Running generate_until requests:  61%|██████▏   | 184/300 [25:43<17:50,  9.23s/it]Running generate_until requests:  62%|██████▏   | 185/300 [25:54<19:03,  9.95s/it]Running generate_until requests:  62%|██████▏   | 186/300 [26:03<18:10,  9.57s/it]Running generate_until requests:  62%|██████▏   | 187/300 [26:06<14:28,  7.69s/it]Running generate_until requests:  63%|██████▎   | 188/300 [26:11<12:41,  6.80s/it]Running generate_until requests:  63%|██████▎   | 189/300 [26:16<11:24,  6.17s/it]Running generate_until requests:  63%|██████▎   | 190/300 [26:21<10:40,  5.83s/it]Running generate_until requests:  64%|██████▎   | 191/300 [26:26<10:34,  5.82s/it]Running generate_until requests:  64%|██████▍   | 192/300 [26:44<16:42,  9.28s/it]Running generate_until requests:  64%|██████▍   | 193/300 [26:54<17:04,  9.57s/it]Running generate_until requests:  65%|██████▍   | 194/300 [27:00<14:57,  8.47s/it]Running generate_until requests:  65%|██████▌   | 195/300 [27:09<15:16,  8.73s/it]Running generate_until requests:  65%|██████▌   | 196/300 [27:16<14:11,  8.19s/it]Running generate_until requests:  66%|██████▌   | 197/300 [27:21<12:09,  7.08s/it]Running generate_until requests:  66%|██████▌   | 198/300 [27:24<10:02,  5.90s/it]Running generate_until requests:  66%|██████▋   | 199/300 [27:31<10:47,  6.41s/it]Running generate_until requests:  67%|██████▋   | 200/300 [27:37<10:20,  6.20s/it]Running generate_until requests:  67%|██████▋   | 201/300 [27:57<16:52, 10.23s/it]Running generate_until requests:  67%|██████▋   | 202/300 [28:03<14:33,  8.91s/it]Running generate_until requests:  68%|██████▊   | 203/300 [28:10<13:42,  8.47s/it]Running generate_until requests:  68%|██████▊   | 204/300 [28:13<10:58,  6.86s/it]Running generate_until requests:  68%|██████▊   | 205/300 [28:25<13:01,  8.22s/it]Running generate_until requests:  69%|██████▊   | 206/300 [28:30<11:46,  7.52s/it]Running generate_until requests:  69%|██████▉   | 207/300 [28:38<11:32,  7.45s/it]Running generate_until requests:  69%|██████▉   | 208/300 [28:47<12:12,  7.96s/it]Running generate_until requests:  70%|██████▉   | 209/300 [28:57<12:54,  8.51s/it]Running generate_until requests:  70%|███████   | 210/300 [29:06<13:14,  8.83s/it]Running generate_until requests:  70%|███████   | 211/300 [29:10<10:58,  7.40s/it]Running generate_until requests:  71%|███████   | 212/300 [29:25<13:52,  9.46s/it]Running generate_until requests:  71%|███████   | 213/300 [29:34<13:43,  9.47s/it]Running generate_until requests:  71%|███████▏  | 214/300 [29:41<12:24,  8.66s/it]Running generate_until requests:  72%|███████▏  | 215/300 [29:46<10:46,  7.61s/it]Running generate_until requests:  72%|███████▏  | 216/300 [29:49<08:48,  6.29s/it]Running generate_until requests:  72%|███████▏  | 217/300 [29:59<10:16,  7.43s/it]Running generate_until requests:  73%|███████▎  | 218/300 [30:09<11:11,  8.19s/it]Running generate_until requests:  73%|███████▎  | 219/300 [30:13<09:21,  6.93s/it]Running generate_until requests:  73%|███████▎  | 220/300 [30:19<08:52,  6.66s/it]Running generate_until requests:  74%|███████▎  | 221/300 [30:39<13:49, 10.50s/it]Running generate_until requests:  74%|███████▍  | 222/300 [30:47<12:43,  9.78s/it]Running generate_until requests:  74%|███████▍  | 223/300 [30:51<10:14,  7.98s/it]Running generate_until requests:  75%|███████▍  | 224/300 [31:04<12:01,  9.50s/it]Running generate_until requests:  75%|███████▌  | 225/300 [31:10<10:33,  8.44s/it]Running generate_until requests:  75%|███████▌  | 226/300 [31:14<08:55,  7.23s/it]Running generate_until requests:  76%|███████▌  | 227/300 [31:26<10:23,  8.54s/it]Running generate_until requests:  76%|███████▌  | 228/300 [31:32<09:27,  7.88s/it]Running generate_until requests:  76%|███████▋  | 229/300 [31:52<13:29, 11.41s/it]Running generate_until requests:  77%|███████▋  | 230/300 [32:00<12:23, 10.63s/it]Running generate_until requests:  77%|███████▋  | 231/300 [32:06<10:24,  9.05s/it]Running generate_until requests:  77%|███████▋  | 232/300 [32:15<10:22,  9.16s/it]Running generate_until requests:  78%|███████▊  | 233/300 [32:23<09:38,  8.64s/it]Running generate_until requests:  78%|███████▊  | 234/300 [32:32<09:53,  8.99s/it]Running generate_until requests:  78%|███████▊  | 235/300 [32:37<08:19,  7.69s/it]Running generate_until requests:  79%|███████▊  | 236/300 [32:47<08:54,  8.35s/it]Running generate_until requests:  79%|███████▉  | 237/300 [32:55<08:40,  8.26s/it]Running generate_until requests:  79%|███████▉  | 238/300 [33:04<08:42,  8.42s/it]Running generate_until requests:  80%|███████▉  | 239/300 [33:12<08:22,  8.25s/it]Running generate_until requests:  80%|████████  | 240/300 [36:06<58:09, 58.15s/it]Running generate_until requests:  80%|████████  | 241/300 [36:11<41:22, 42.07s/it]Running generate_until requests:  81%|████████  | 242/300 [36:16<29:50, 30.88s/it]Running generate_until requests:  81%|████████  | 243/300 [36:26<23:31, 24.76s/it]Running generate_until requests:  81%|████████▏ | 244/300 [36:30<17:16, 18.51s/it]Running generate_until requests:  82%|████████▏ | 245/300 [36:50<17:22, 18.96s/it]Running generate_until requests:  82%|████████▏ | 246/300 [36:54<13:02, 14.50s/it]Running generate_until requests:  82%|████████▏ | 247/300 [36:59<10:22, 11.74s/it]Running generate_until requests:  83%|████████▎ | 248/300 [37:05<08:39,  9.99s/it]Running generate_until requests:  83%|████████▎ | 249/300 [37:11<07:26,  8.76s/it]Running generate_until requests:  83%|████████▎ | 250/300 [37:26<08:43, 10.47s/it]Running generate_until requests:  84%|████████▎ | 251/300 [37:32<07:29,  9.17s/it]Running generate_until requests:  84%|████████▍ | 252/300 [37:38<06:40,  8.34s/it]Running generate_until requests:  84%|████████▍ | 253/300 [37:51<07:34,  9.66s/it]Running generate_until requests:  85%|████████▍ | 254/300 [37:58<06:47,  8.86s/it]Running generate_until requests:  85%|████████▌ | 255/300 [38:01<05:27,  7.29s/it]Running generate_until requests:  85%|████████▌ | 256/300 [38:11<05:49,  7.94s/it]Running generate_until requests:  86%|████████▌ | 257/300 [38:15<04:53,  6.82s/it]Running generate_until requests:  86%|████████▌ | 258/300 [38:24<05:10,  7.40s/it]Running generate_until requests:  86%|████████▋ | 259/300 [38:35<05:48,  8.51s/it]Running generate_until requests:  87%|████████▋ | 260/300 [38:43<05:37,  8.44s/it]Running generate_until requests:  87%|████████▋ | 261/300 [38:50<05:08,  7.91s/it]Running generate_until requests:  87%|████████▋ | 262/300 [38:53<04:00,  6.33s/it]Running generate_until requests:  88%|████████▊ | 263/300 [39:01<04:14,  6.87s/it]Running generate_until requests:  88%|████████▊ | 264/300 [39:13<05:07,  8.55s/it]Running generate_until requests:  88%|████████▊ | 265/300 [39:17<04:09,  7.14s/it]Running generate_until requests:  89%|████████▊ | 266/300 [39:23<03:49,  6.75s/it]Running generate_until requests:  89%|████████▉ | 267/300 [39:31<03:59,  7.26s/it]Running generate_until requests:  89%|████████▉ | 268/300 [39:33<03:00,  5.64s/it]Running generate_until requests:  90%|████████▉ | 269/300 [39:38<02:43,  5.29s/it]Running generate_until requests:  90%|█████████ | 270/300 [39:56<04:33,  9.12s/it]Running generate_until requests:  90%|█████████ | 271/300 [40:05<04:23,  9.07s/it]Running generate_until requests:  91%|█████████ | 272/300 [40:13<04:07,  8.86s/it]Running generate_until requests:  91%|█████████ | 273/300 [40:21<03:55,  8.72s/it]Running generate_until requests:  91%|█████████▏| 274/300 [40:27<03:19,  7.67s/it]Running generate_until requests:  92%|█████████▏| 275/300 [40:32<02:51,  6.87s/it]Running generate_until requests:  92%|█████████▏| 276/300 [40:45<03:33,  8.88s/it]Running generate_until requests:  92%|█████████▏| 277/300 [40:49<02:46,  7.23s/it]Running generate_until requests:  93%|█████████▎| 278/300 [40:56<02:42,  7.39s/it]Running generate_until requests:  93%|█████████▎| 279/300 [41:04<02:38,  7.57s/it]Running generate_until requests:  93%|█████████▎| 280/300 [41:15<02:47,  8.37s/it]Running generate_until requests:  94%|█████████▎| 281/300 [41:19<02:17,  7.26s/it]Running generate_until requests:  94%|█████████▍| 282/300 [41:28<02:20,  7.83s/it]Running generate_until requests:  94%|█████████▍| 283/300 [41:36<02:11,  7.73s/it]Running generate_until requests:  95%|█████████▍| 284/300 [41:42<01:57,  7.37s/it]Running generate_until requests:  95%|█████████▌| 285/300 [41:49<01:47,  7.16s/it]Running generate_until requests:  95%|█████████▌| 286/300 [41:55<01:34,  6.76s/it]Running generate_until requests:  96%|█████████▌| 287/300 [42:02<01:27,  6.71s/it]Running generate_until requests:  96%|█████████▌| 288/300 [42:07<01:14,  6.19s/it]Running generate_until requests:  96%|█████████▋| 289/300 [42:13<01:08,  6.22s/it]Running generate_until requests:  97%|█████████▋| 290/300 [42:18<00:57,  5.76s/it]Running generate_until requests:  97%|█████████▋| 291/300 [42:37<01:29,  9.94s/it]Running generate_until requests:  97%|█████████▋| 292/300 [42:40<01:03,  7.90s/it]Running generate_until requests:  98%|█████████▊| 293/300 [42:56<01:11, 10.25s/it]Running generate_until requests:  98%|█████████▊| 294/300 [43:01<00:52,  8.72s/it]Running generate_until requests:  98%|█████████▊| 295/300 [43:13<00:47,  9.52s/it]Running generate_until requests:  99%|█████████▊| 296/300 [43:32<00:50, 12.57s/it]Running generate_until requests:  99%|█████████▉| 297/300 [43:39<00:32, 10.68s/it]Running generate_until requests:  99%|█████████▉| 298/300 [43:44<00:18,  9.12s/it]Running generate_until requests: 100%|█████████▉| 299/300 [43:52<00:08,  8.63s/it]Running generate_until requests: 100%|██████████| 300/300 [43:57<00:00,  7.77s/it]Running generate_until requests: 100%|██████████| 300/300 [43:57<00:00,  8.79s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
finish_task gsm8k
|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value |   |Stderr|
|-----|------:|----------------|-----:|-----------|---|-----:|---|-----:|
|gsm8k|      3|flexible-extract|     5|exact_match|↑  |0.5167|±  |0.0289|
|     |       |strict-match    |     5|exact_match|↑  |0.5167|±  |0.0289|

========================================
end : sp_config:  (0.0, 0.0, 0.0, 0) num_shot:  5
results
|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value |   |Stderr|
|-----|------:|----------------|-----:|-----------|---|-----:|---|-----:|
|gsm8k|      3|flexible-extract|     5|exact_match|↑  |0.5167|±  |0.0289|
|     |       |strict-match    |     5|exact_match|↑  |0.5167|±  |0.0289|

========================================
