
  0%|                                                                                                                                                           | 0/9 [00:00<?, ?it/s]/home/donglinbai/miniconda3/envs/sparse/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Traceback (most recent call last):
  File "/data/wzw/wzw/BitDistiller-Q4_0/train/train_single.py", line 179, in <module>
    train()
  File "/data/wzw/wzw/BitDistiller-Q4_0/train/train_single.py", line 172, in train
    trainer.train()
  File "/home/donglinbai/miniconda3/envs/sparse/lib/python3.9/site-packages/transformers/trainer.py", line 2241, in train
    return inner_training_loop(
  File "/home/donglinbai/miniconda3/envs/sparse/lib/python3.9/site-packages/transformers/trainer.py", line 2548, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/home/donglinbai/miniconda3/envs/sparse/lib/python3.9/site-packages/transformers/trainer.py", line 3698, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
  File "/home/donglinbai/miniconda3/envs/sparse/lib/python3.9/site-packages/transformers/trainer.py", line 3759, in compute_loss
    outputs = model(**inputs)
  File "/home/donglinbai/miniconda3/envs/sparse/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/donglinbai/miniconda3/envs/sparse/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/donglinbai/miniconda3/envs/sparse/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py", line 186, in forward
    return self.gather(outputs, self.output_device)
  File "/home/donglinbai/miniconda3/envs/sparse/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py", line 203, in gather
    return gather(outputs, output_device, dim=self.dim)
  File "/home/donglinbai/miniconda3/envs/sparse/lib/python3.9/site-packages/torch/nn/parallel/scatter_gather.py", line 105, in gather
    res = gather_map(outputs)
  File "/home/donglinbai/miniconda3/envs/sparse/lib/python3.9/site-packages/torch/nn/parallel/scatter_gather.py", line 96, in gather_map
    return type(out)((k, gather_map([d[k] for d in outputs]))
  File "<string>", line 8, in __init__
  File "/home/donglinbai/miniconda3/envs/sparse/lib/python3.9/site-packages/transformers/utils/generic.py", line 392, in __post_init__
    for idx, element in enumerate(iterator):
  File "/home/donglinbai/miniconda3/envs/sparse/lib/python3.9/site-packages/torch/nn/parallel/scatter_gather.py", line 96, in <genexpr>
    return type(out)((k, gather_map([d[k] for d in outputs]))
  File "/home/donglinbai/miniconda3/envs/sparse/lib/python3.9/site-packages/torch/nn/parallel/scatter_gather.py", line 90, in gather_map
    return Gather.apply(target_device, dim, *outputs)
  File "/home/donglinbai/miniconda3/envs/sparse/lib/python3.9/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/donglinbai/miniconda3/envs/sparse/lib/python3.9/site-packages/torch/nn/parallel/_functions.py", line 75, in forward
    return comm.gather(inputs, ctx.dim, ctx.target_device)
  File "/home/donglinbai/miniconda3/envs/sparse/lib/python3.9/site-packages/torch/nn/parallel/comm.py", line 231, in gather
    return torch._C._gather(tensors, dim, destination)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.64 GiB. GPU 0 has a total capacty of 23.64 GiB of which 2.40 GiB is free. Process 782723 has 488.00 MiB memory in use. Including non-PyTorch memory, this process has 20.75 GiB memory in use. Of the allocated memory 19.50 GiB is allocated by PyTorch, and 754.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/data/wzw/wzw/BitDistiller-Q4_0/train/train_single.py", line 179, in <module>
    train()
  File "/data/wzw/wzw/BitDistiller-Q4_0/train/train_single.py", line 172, in train
    trainer.train()
  File "/home/donglinbai/miniconda3/envs/sparse/lib/python3.9/site-packages/transformers/trainer.py", line 2241, in train
    return inner_training_loop(
  File "/home/donglinbai/miniconda3/envs/sparse/lib/python3.9/site-packages/transformers/trainer.py", line 2548, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/home/donglinbai/miniconda3/envs/sparse/lib/python3.9/site-packages/transformers/trainer.py", line 3698, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
  File "/home/donglinbai/miniconda3/envs/sparse/lib/python3.9/site-packages/transformers/trainer.py", line 3759, in compute_loss
    outputs = model(**inputs)
  File "/home/donglinbai/miniconda3/envs/sparse/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/donglinbai/miniconda3/envs/sparse/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/donglinbai/miniconda3/envs/sparse/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py", line 186, in forward
    return self.gather(outputs, self.output_device)
  File "/home/donglinbai/miniconda3/envs/sparse/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py", line 203, in gather
    return gather(outputs, output_device, dim=self.dim)
  File "/home/donglinbai/miniconda3/envs/sparse/lib/python3.9/site-packages/torch/nn/parallel/scatter_gather.py", line 105, in gather
    res = gather_map(outputs)
  File "/home/donglinbai/miniconda3/envs/sparse/lib/python3.9/site-packages/torch/nn/parallel/scatter_gather.py", line 96, in gather_map
    return type(out)((k, gather_map([d[k] for d in outputs]))
  File "<string>", line 8, in __init__
  File "/home/donglinbai/miniconda3/envs/sparse/lib/python3.9/site-packages/transformers/utils/generic.py", line 392, in __post_init__
    for idx, element in enumerate(iterator):
  File "/home/donglinbai/miniconda3/envs/sparse/lib/python3.9/site-packages/torch/nn/parallel/scatter_gather.py", line 96, in <genexpr>
    return type(out)((k, gather_map([d[k] for d in outputs]))
  File "/home/donglinbai/miniconda3/envs/sparse/lib/python3.9/site-packages/torch/nn/parallel/scatter_gather.py", line 90, in gather_map
    return Gather.apply(target_device, dim, *outputs)
  File "/home/donglinbai/miniconda3/envs/sparse/lib/python3.9/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/donglinbai/miniconda3/envs/sparse/lib/python3.9/site-packages/torch/nn/parallel/_functions.py", line 75, in forward
    return comm.gather(inputs, ctx.dim, ctx.target_device)
  File "/home/donglinbai/miniconda3/envs/sparse/lib/python3.9/site-packages/torch/nn/parallel/comm.py", line 231, in gather
    return torch._C._gather(tensors, dim, destination)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.64 GiB. GPU 0 has a total capacty of 23.64 GiB of which 2.40 GiB is free. Process 782723 has 488.00 MiB memory in use. Including non-PyTorch memory, this process has 20.75 GiB memory in use. Of the allocated memory 19.50 GiB is allocated by PyTorch, and 754.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF