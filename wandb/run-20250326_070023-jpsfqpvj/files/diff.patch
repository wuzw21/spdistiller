diff --git a/tools/amlt_wrapper.sh b/tools/amlt_wrapper.sh
index 087f692..a6dde20 100644
--- a/tools/amlt_wrapper.sh
+++ b/tools/amlt_wrapper.sh
@@ -5,7 +5,7 @@ source tools/params_temp.env
 export CURRENT_DIR=$(dirname $(dirname $(realpath "$0")))
 
 if [ -z ${AMLT_MAP_INPUT_DIR} ]; then
-    export MODEL_DIR=${AMLT_MAP_INPUT_DIR}/ckpts/${MODEL_NAME}
+    export MODEL_DIR=${AMLT_MAP_INPUT_DIR}/ckpts
 else
     export MODEL_DIR=${AMLT_DATA_DIR}/models
 fi
diff --git a/tools/run_wrapper.sh b/tools/run_wrapper.sh
index 27d02cb..171291d 100644
--- a/tools/run_wrapper.sh
+++ b/tools/run_wrapper.sh
@@ -4,7 +4,7 @@ source tools/params_temp.env
 
 export CURRENT_DIR=$(dirname $(dirname $(realpath "$0")))
 # change this to your model directory
-export MODEL_DIR=/data/wzw/models/${MODEL_NAME}
+export MODEL_DIR=/data/wzw/models
 export OUTPUT_DIR=/data/wzw/Projects/bitdistiller
 export DATA_DIR=$CURRENT_DIR/data
 
diff --git a/tools/train.sh b/tools/train.sh
index 01c781c..c816bea 100644
--- a/tools/train.sh
+++ b/tools/train.sh
@@ -2,7 +2,7 @@
 
 cd train
 
-bash train.sh \
+bash train_local.sh \
     ${DATA_DIR}/datasets/${MODEL_NAME}/${DATASET} \
     ${OUTPUT_DIR}/ckpts/${MODEL_NAME}/ \
     ${OUTPUT_DIR}/logs/${MODEL_NAME}/
diff --git a/train/train.py b/train/train.py
index ff203f0..3289cfa 100644
--- a/train/train.py
+++ b/train/train.py
@@ -221,7 +221,7 @@ def get_int_from_envs(name):
     return int(var) if var is not None else 0
 
 def get_float_from_envs(name):
-    var = os.environ.get(name)
+    var = os.environ.get(name)  
     return float(var) if var is not None else 0
 
 import math
@@ -257,13 +257,13 @@ def train():
     if training_args.use_lora:
         model = prepare_model_for_kbit_training(model)
     # prepare sparse
-    prepare_sparse_hook(model)
-    global_weight_preditor = model.predictor
-    if global_weight_preditor is not None:
-        attn_sp, mlp_sp, w_p, do_cr = get_sparsity_configs()
-        global_weight_preditor.set_sp_config(attn_sp, mlp_sp, w_p)
-        global_weight_preditor.set_do_pre_prediction(do_cr)
-        global_weight_preditor.set_sparsity_threshold(data_args.threshold_path)
+    # prepare_sparse_hook(model)
+    # global_weight_preditor = model.predictor
+    # if global_weight_preditor is not None:
+    #     attn_sp, mlp_sp, w_p, do_cr = get_sparsity_configs()
+    #     global_weight_preditor.set_sp_config(attn_sp, mlp_sp, w_p)
+    #     global_weight_preditor.set_do_pre_prediction(do_cr)
+    #     global_weight_preditor.set_sparsity_threshold(data_args.threshold_path)
 
     tokenizer = transformers.AutoTokenizer.from_pretrained(
         model_args.model_name_or_path,
@@ -385,7 +385,7 @@ def train():
     #     param.requires_grad = True
     resume_ckpt = get_last_checkpoint(training_args.output_dir)
     print('resume_ckpt', resume_ckpt)
-    trainer.train(resume_from_checkpoint=resume_ckpt)
+    trainer.train(resume_from_checkpoint=None)
 
     safe_save_model_for_hf_trainer(trainer=trainer, output_dir=training_args.output_dir)
 
diff --git a/train/train.sh b/train/train.sh
index 242fe4b..a1cb329 100644
--- a/train/train.sh
+++ b/train/train.sh
@@ -46,4 +46,4 @@ deepspeed --no_ssh --node_rank=0 \
     --max_train_samples 999999 \
     --evaluation_strategy "steps" \
     --eval_steps  100 \
-    --use_lora False
\ No newline at end of file
+    --use_lora True
\ No newline at end of file
diff --git a/train/train_python.sh b/train/train_python.sh
deleted file mode 100644
index 36812f4..0000000
--- a/train/train_python.sh
+++ /dev/null
@@ -1,73 +0,0 @@
-
-export DATA_PATH=$1
-export SAVE_PATH=$2
-export LOGGING_DIR=$3
-export NUM_TRAIN_EPOCHS=$4
-export MASTER_ADDR="localhost"
-export MASTER_PORT="1321"
-export GLOO_SOCKET_IFNAME="lo"
-export NCCL_SOCKET_IFNAME="lo"
-export WANDB_DISABLED=true
-
-export MODEL_PATH=$5
-export MODEL_NAME=$6
-
-#rm -rf /job/hostfile
-
-# No ssh
-#--hostfile=hostfile_remote --no_ssh --node_rank=0
-
-# --clip BitDistiller/quantization/clip_cache/WizardCoder-7B/7b-int2-g128-twoclip.pt
-# --evaluation_strategy "steps"
-# --eval_steps 4
-# --bits 4 --quant_type Q4_0 --q_group_size 64
-quant_type=Q4_0
-bits=4
-# quant_type=ste-n2f3
-# bits=3
-
-python train.py \
-    --model_name_or_path ${MODEL_PATH} \
-    --data_path ${DATA_PATH} \
-    --threshold_path ${THRESHOLD_PATH} \
-    --model_max_length 512 \
-    --output_dir ${SAVE_PATH} \
-    --logging_dir ${LOGGING_DIR} \
-    --num_train_epochs ${NUM_TRAIN_EPOCHS} \
-    --bf16 True \
-    --seed 42 \
-    --per_device_train_batch_size 4 \
-    --per_device_eval_batch_size 4 \
-    --gradient_accumulation_steps 8 \
-    --gradient_checkpointing True \
-    --load_best_model_at_end False \
-    --save_strategy "epoch" \
-    --save_total_limit 1 \
-    --learning_rate 8e-6 \
-    --lr_scheduler_type "constant" \
-    --weight_decay 0. \
-    --logging_steps 1 \
-    --report_to "none" \
-    --deepspeed config/zero.json \
-    --bits ${bits} \
-    --quant_type ${quant_type} \
-    --q_group_size 64 \
-    --train_kd False \
-    --kd_loss_type "cakld" \
-    --max_train_samples 999999 \
-    --max_memory ${MAX_MEMORY} \
-    --evaluation_strategy "steps" \
-    --eval_steps 2000 \
-    --use_lora True
-
-cd ..
-
-cd test/general
-
-python wiki_ppl.py --model ${SAVE_PATH} --quant_type int --bits ${bits} --group_size 128
-
-cd ..
-
-cd ..
-
-cd train
\ No newline at end of file
