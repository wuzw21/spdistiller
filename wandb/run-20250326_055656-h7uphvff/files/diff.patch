diff --git a/tools/train.sh b/tools/train.sh
index 01c781c..c816bea 100644
--- a/tools/train.sh
+++ b/tools/train.sh
@@ -2,7 +2,7 @@
 
 cd train
 
-bash train.sh \
+bash train_local.sh \
     ${DATA_DIR}/datasets/${MODEL_NAME}/${DATASET} \
     ${OUTPUT_DIR}/ckpts/${MODEL_NAME}/ \
     ${OUTPUT_DIR}/logs/${MODEL_NAME}/
diff --git a/train/train.py b/train/train.py
index ff203f0..965ec1c 100644
--- a/train/train.py
+++ b/train/train.py
@@ -257,13 +257,13 @@ def train():
     if training_args.use_lora:
         model = prepare_model_for_kbit_training(model)
     # prepare sparse
-    prepare_sparse_hook(model)
-    global_weight_preditor = model.predictor
-    if global_weight_preditor is not None:
-        attn_sp, mlp_sp, w_p, do_cr = get_sparsity_configs()
-        global_weight_preditor.set_sp_config(attn_sp, mlp_sp, w_p)
-        global_weight_preditor.set_do_pre_prediction(do_cr)
-        global_weight_preditor.set_sparsity_threshold(data_args.threshold_path)
+    # prepare_sparse_hook(model)
+    # global_weight_preditor = model.predictor
+    # if global_weight_preditor is not None:
+    #     attn_sp, mlp_sp, w_p, do_cr = get_sparsity_configs()
+    #     global_weight_preditor.set_sp_config(attn_sp, mlp_sp, w_p)
+    #     global_weight_preditor.set_do_pre_prediction(do_cr)
+    #     global_weight_preditor.set_sparsity_threshold(data_args.threshold_path)
 
     tokenizer = transformers.AutoTokenizer.from_pretrained(
         model_args.model_name_or_path,
@@ -385,7 +385,7 @@ def train():
     #     param.requires_grad = True
     resume_ckpt = get_last_checkpoint(training_args.output_dir)
     print('resume_ckpt', resume_ckpt)
-    trainer.train(resume_from_checkpoint=resume_ckpt)
+    trainer.train(resume_from_checkpoint=None)
 
     safe_save_model_for_hf_trainer(trainer=trainer, output_dir=training_args.output_dir)
 
diff --git a/train/train.sh b/train/train.sh
index 242fe4b..a1cb329 100644
--- a/train/train.sh
+++ b/train/train.sh
@@ -46,4 +46,4 @@ deepspeed --no_ssh --node_rank=0 \
     --max_train_samples 999999 \
     --evaluation_strategy "steps" \
     --eval_steps  100 \
-    --use_lora False
\ No newline at end of file
+    --use_lora True
\ No newline at end of file
diff --git a/train/train_local.py b/train/train_local.py
index 798d02e..0c507b4 100644
--- a/train/train_local.py
+++ b/train/train_local.py
@@ -16,10 +16,8 @@ import torch.nn as nn
 import transformers
 from torch.utils.data import Dataset, DataLoader
 from transformers import Trainer, BitsAndBytesConfig, default_data_collator
-from datasets import load_dataset
 import json
 import glob
-import torch.distributed as dist
 
 from accelerate import init_empty_weights, infer_auto_device_map, dispatch_model
 from mytrainer import KDTrainer
@@ -28,7 +26,6 @@ from tqdm import tqdm
 from datasets import load_dataset
 
 from utils.sparse_hook import prepare_sparse_hook, get_sparsity_configs
-
 # 如果需要使用LoRA，则导入PEFT相关模块
 from peft import LoraConfig, get_peft_model,prepare_model_for_kbit_training
 
@@ -89,7 +86,6 @@ class TrainingArguments(transformers.TrainingArguments):
     kd_tmp: int = field(default=1, metadata={"help": "Temperature of KD"})
     kd_loss_type: str = field(default=None, metadata={"help": "Type of loss function when KD-QAT"})
     cakld_steps: int = field(default=10, metadata={"help": "How many steps to calculate the coefficient of CAKLD."})
-    max_memory: str = field(default="80000MB", metadata={"help": "max_memory for cuda device"})
     evaluation_strategy: str = field(
         default="steps",
         metadata={"help": "Evaluation strategy to adopt during training. Options: 'no', 'steps', 'epoch'."}
@@ -168,9 +164,10 @@ class SupervisedDataset(Dataset):
         if split == "train":
             self.sources, self.targets = self.sources[split_num:], self.targets[split_num:]
             print(f"Using {len(self.sources)} samples to train")
-            print("Example Data")
-            print("sources: \n", self.sources[0])
-            print("targets: \n", self.targets[0])
+            # print("Example Data")
+            # print("sources: \n", self.sources[0])
+            # print("targets: \n", self.targets[0])
+            # print('fix_finish')
         elif split == "eval":
             self.sources, self.targets = self.sources[:split_num], self.targets[:split_num]
             print(f"Using {len(self.sources)} samples to evaluation")
@@ -240,44 +237,24 @@ def compute_metrics(eval_pred):
     return {"ppl": ppl}
 
 def train():
-    # try:
-    #     print(f"ROCm是否可用: {torch.cuda.is_available()}")
-    #     print(f"设备数量: {torch.cuda.device_count()}")
-    #     print(f"当前设备: {torch.cuda.current_device()}")
-    #     print(f"设备名称: {torch.cuda.get_device_name(0)}")
-    #     print(torch.__version__)
-    #     print(torch.cuda.is_available())
-    #     print(torch.version.hip)
-    # except KeyError as e:
-    #     pass
-
     parser = transformers.HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))
     model_args, data_args, training_args = parser.parse_args_into_dataclasses()
     random.seed(training_args.seed)
-    n_gpus = torch.cuda.device_count()
-    max_memory = training_args.max_memory
-    max_memory = {i: max_memory for i in range(n_gpus)}
-    device_map = "auto"
-    if os.environ.get('LOCAL_RANK') is not None:
-        local_rank = int(os.environ.get('LOCAL_RANK', '0'))
-        device_map = {'': local_rank}
-        max_memory = {'': max_memory[local_rank]}
-    print('max_memory:', max_memory, "device_map", device_map)
     print(f"loading {model_args.model_name_or_path} model")
     model = transformers.AutoModelForCausalLM.from_pretrained(
         model_args.model_name_or_path,
         torch_dtype=torch.bfloat16,
-        device_map=None,
     )
-    model = prepare_model_for_kbit_training(model)
+    if training_args.use_lora:
+        model = prepare_model_for_kbit_training(model)
     # prepare sparse
-    prepare_sparse_hook(model)
-    global_weight_preditor = model.predictor
-    if global_weight_preditor is not None:
-        attn_sp, mlp_sp, w_p, do_cr = get_sparsity_configs()
-        global_weight_preditor.set_sp_config(attn_sp, mlp_sp, w_p)
-        global_weight_preditor.set_do_pre_prediction(do_cr)
-        global_weight_preditor.set_sparsity_threshold(data_args.threshold_path)
+    # prepare_sparse_hook(model)
+    # global_weight_preditor = model.predictor
+    # if global_weight_preditor is not None:
+    #     attn_sp, mlp_sp, w_p, do_cr = get_sparsity_configs()
+    #     global_weight_preditor.set_sp_config(attn_sp, mlp_sp, w_p)
+    #     global_weight_preditor.set_do_pre_prediction(do_cr)
+    #     global_weight_preditor.set_sparsity_threshold(data_args.threshold_path)
 
     tokenizer = transformers.AutoTokenizer.from_pretrained(
         model_args.model_name_or_path,
@@ -302,17 +279,6 @@ def train():
             "unk_token": DEFAULT_UNK_TOKEN,
         })
     data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)
-
-    # load quantization if specified
-    if training_args.quant_type is not None:
-        print("converting the model to qat, this may take a while...")
-        # model, _ = convertModelToQuant(model, compute_dtype=torch.bfloat16, quant_type=training_args.quant_type, q_group_size=training_args.q_group_size)
-    if training_args.clip is not None:
-        # q_config = {"zero_point": True, "q_group_size": training_args.q_group_size}
-        # print("Loading pre-computed Clipping results from", training_args.clip)
-        # clip_results = torch.load(training_args.clip)
-        # apply_clip(model, clip_results)
-        print("Clipping init successfully!")
         
     # 如果需要进行LoRA微调，则包装模型
     if training_args.use_lora:
@@ -330,69 +296,16 @@ def train():
 
         print("Applying LoRA fine-tuning...")
     
-    # load teacher model if KD training is enabled (assume LoRA and KD are mutually exclusive)
-    if training_args.train_kd:
-        print("loading Teacher Model...")
-        teacher_model = transformers.AutoModelForCausalLM.from_pretrained(
-            model_args.model_name_or_path,
-            load_in_4bit=False,
-            load_in_8bit=False,
-            torch_dtype=torch.bfloat16,
-            device_map=device_map,
-            max_memory=max_memory,
-        )
-        teacher_model.eval()
-        teacher_model.cuda()
-        for param in teacher_model.parameters():
-            param.requires_grad = False
-        teacher_model.config.use_cache = False
-        if pad_status is False:
-            smart_tokenizer_and_embedding_resize(
-                special_tokens_dict=dict(pad_token=DEFAULT_PAD_TOKEN),
-                tokenizer=tokenizer,
-                model=teacher_model,
-            )
-        model.kd_loss_scale = 1.0
-        print("Teacher Model loaded")
-
-    # compute cakld coefficient if needed
-    mean_prob = 0
-    if training_args.train_kd and training_args.kd_loss_type == "cakld":
-        print("Get the main Prob!")
-        probDataloader = DataLoader(
-            data_module['train_dataset'], 
-            shuffle=True, 
-            collate_fn=data_module['data_collator'], 
-            batch_size=training_args.per_device_train_batch_size,
-            drop_last=True,
-        )
-        prob = 0
-        for step, batch in tqdm(enumerate(probDataloader)):
-            if step > training_args.cakld_steps:
-                break
-            batch = {k: v.to(teacher_model.device) for k, v in batch.items()}
-            with torch.no_grad():
-                outputs = teacher_model(**batch)
-            logits = outputs.get("logits").contiguous()
-            prob1 = torch.nn.functional.softmax(logits, dim=-1)
-            prob1 = torch.max(prob1, dim=-1).values 
-            prob += prob1.mean()
-        mean_prob = prob / training_args.cakld_steps
-        mean_prob = torch.Tensor(mean_prob.to(teacher_model.device))
-        dist.all_reduce(mean_prob, op=dist.ReduceOp.SUM)
-        mean_prob = mean_prob / dist.get_world_size()
-        print(f"Get the coefficient: {mean_prob}")
-
-    # load trainer (这里假设LoRA和KD微调互斥)
     if training_args.train_kd:
         trainer = KDTrainer(model=model, tokenizer=tokenizer, teacher_model=teacher_model, loss_type=training_args.kd_loss_type, mean_prob=mean_prob, args=training_args, compute_metrics=compute_metrics, **data_module)
     else:
         trainer = Trainer(model=model, tokenizer=tokenizer, args=training_args, compute_metrics=compute_metrics, **data_module)
 
     resume_ckpt = get_last_checkpoint(training_args.output_dir)
-    trainer.train(resume_from_checkpoint=resume_ckpt)
+    print('resume_ckpt', resume_ckpt)
+    trainer.train(resume_from_checkpoint=None)
 
     safe_save_model_for_hf_trainer(trainer=trainer, output_dir=training_args.output_dir)
 
 if __name__ == "__main__":
-    train()
+    train()
\ No newline at end of file
diff --git a/train/train_python.sh b/train/train_python.sh
deleted file mode 100644
index 36812f4..0000000
--- a/train/train_python.sh
+++ /dev/null
@@ -1,73 +0,0 @@
-
-export DATA_PATH=$1
-export SAVE_PATH=$2
-export LOGGING_DIR=$3
-export NUM_TRAIN_EPOCHS=$4
-export MASTER_ADDR="localhost"
-export MASTER_PORT="1321"
-export GLOO_SOCKET_IFNAME="lo"
-export NCCL_SOCKET_IFNAME="lo"
-export WANDB_DISABLED=true
-
-export MODEL_PATH=$5
-export MODEL_NAME=$6
-
-#rm -rf /job/hostfile
-
-# No ssh
-#--hostfile=hostfile_remote --no_ssh --node_rank=0
-
-# --clip BitDistiller/quantization/clip_cache/WizardCoder-7B/7b-int2-g128-twoclip.pt
-# --evaluation_strategy "steps"
-# --eval_steps 4
-# --bits 4 --quant_type Q4_0 --q_group_size 64
-quant_type=Q4_0
-bits=4
-# quant_type=ste-n2f3
-# bits=3
-
-python train.py \
-    --model_name_or_path ${MODEL_PATH} \
-    --data_path ${DATA_PATH} \
-    --threshold_path ${THRESHOLD_PATH} \
-    --model_max_length 512 \
-    --output_dir ${SAVE_PATH} \
-    --logging_dir ${LOGGING_DIR} \
-    --num_train_epochs ${NUM_TRAIN_EPOCHS} \
-    --bf16 True \
-    --seed 42 \
-    --per_device_train_batch_size 4 \
-    --per_device_eval_batch_size 4 \
-    --gradient_accumulation_steps 8 \
-    --gradient_checkpointing True \
-    --load_best_model_at_end False \
-    --save_strategy "epoch" \
-    --save_total_limit 1 \
-    --learning_rate 8e-6 \
-    --lr_scheduler_type "constant" \
-    --weight_decay 0. \
-    --logging_steps 1 \
-    --report_to "none" \
-    --deepspeed config/zero.json \
-    --bits ${bits} \
-    --quant_type ${quant_type} \
-    --q_group_size 64 \
-    --train_kd False \
-    --kd_loss_type "cakld" \
-    --max_train_samples 999999 \
-    --max_memory ${MAX_MEMORY} \
-    --evaluation_strategy "steps" \
-    --eval_steps 2000 \
-    --use_lora True
-
-cd ..
-
-cd test/general
-
-python wiki_ppl.py --model ${SAVE_PATH} --quant_type int --bits ${bits} --group_size 128
-
-cd ..
-
-cd ..
-
-cd train
\ No newline at end of file
