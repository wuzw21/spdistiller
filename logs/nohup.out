dir ../datasets/Meta-Llama-3-8B has existed
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00, 13.31it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 13.81it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 13.72it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
only use 5000 samples
  0%|          | 0/312.5 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  0%|          | 1/312.5 [37:42<195:46:09, 2262.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  1%|          | 2/312.5 [1:13:58<190:45:28, 2211.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  1%|          | 3/312.5 [1:44:27<175:07:54, 2037.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  1%|▏         | 4/312.5 [2:17:48<173:20:08, 2022.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
  2%|▏         | 5/312.5 [2:48:12<166:39:12, 1951.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
generate.sh: line 25: 1200261 Killed                  python single_generate.py --base_model ${MODEL_DIR} --dataset_name ${DATASET} --out_path ${OUTPUT} --batch_size ${BATCH_SIZE} --max_sample ${MAX_SAMPLE}
dir ../datasets/Meta-Llama-3-8B/ has existed
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00, 12.36it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 13.26it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 13.11it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
generate.sh: line 25: 1300911 Killed                  python single_generate.py --base_model ${MODEL_DIR} --dataset_name ${DATASET} --out_path ${OUTPUT} --batch_size ${BATCH_SIZE} --max_sample ${MAX_SAMPLE}
